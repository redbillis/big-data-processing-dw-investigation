{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cc02ea",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation\n",
    "\n",
    "This notebook trains and evaluates regression models to predict SalesAmount using the processed, feature-engineered dataset prepared in the previous step.\n",
    "\n",
    "The goal is to:\n",
    "- Establish a baseline performance\n",
    "- Compare multiple regression models\n",
    "- Evaluate generalization performance\n",
    "- Identify key drivers of sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d435b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# Define processed data directory\n",
    "processed_dir = os.path.join(\"..\", \"data\", \"processed\")\n",
    "\n",
    "# Load preprocessed datasets\n",
    "X_train = pd.read_csv(os.path.join(processed_dir, \"X_train.csv\"))\n",
    "X_test = pd.read_csv(os.path.join(processed_dir, \"X_test.csv\"))\n",
    "y_train = pd.read_csv(os.path.join(processed_dir, \"y_train.csv\")).squeeze()\n",
    "y_test = pd.read_csv(os.path.join(processed_dir, \"y_test.csv\")).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cdc35",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Before training more complex models, we establish a baseline performance using a simple regression strategy.\n",
    "\n",
    "The baseline model predicts the mean of the target variable regardless of input features. While simplistic, it provides a minimum performance threshold that all subsequent models should outperform.\n",
    "\n",
    "This ensures that any added model complexity is justified by measurable performance gains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9631e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize baseline regressor (predicts mean of y_train)\n",
    "baseline_model = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "# Fit baseline model\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29cd910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(694.7562612966226, np.float64(931.0226674689202), -2.8972993177500683e-05)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate baseline performance\n",
    "baseline_mae = mean_absolute_error(y_test, y_pred_baseline)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "baseline_mae, baseline_rmse, baseline_r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4969fa",
   "metadata": {},
   "source": [
    "## Baseline Performance Summary\n",
    "\n",
    "The baseline model provides a reference point for model evaluation.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- MAE and RMSE reflect error magnitude when predicting the average sale value\n",
    "- R² is expected to be close to 0, indicating no explanatory power\n",
    "- Any predictive model must significantly outperform this baseline to be considered useful\n",
    "\n",
    "The baseline results will be used as a benchmark when evaluating trained regression models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641603a",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We now train a Linear Regression model to establish a first meaningful predictive benchmark beyond the baseline model.\n",
    "\n",
    "Linear Regression learns a coefficient (weight) for each feature and combines them linearly to predict the target variable (`SalesAmount`).\n",
    "\n",
    "This step allows us to:\n",
    "- Verify that the engineered features contain predictive signal\n",
    "- Quantify improvement over the baseline\n",
    "- Establish a reference for more advanced models\n",
    "\n",
    "If Linear Regression fails to outperform the baseline, it indicates issues in feature engineering, preprocessing, or data leakage handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580602e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Linear Regression model\n",
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "# Train model on training data\n",
    "lin_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_lr = lin_reg_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d208232c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016980434102387105, np.float64(0.06982823170490929), 0.9999999943745902)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Linear Regression performance\n",
    "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "lr_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "lr_mae, lr_rmse, lr_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb7315",
   "metadata": {},
   "source": [
    "## Linear Regression Performance Summary\n",
    "\n",
    "The Linear Regression model provides the first true test of whether the engineered feature set can explain variation in `SalesAmount`.\n",
    "\n",
    "Evaluation metrics are interpreted as follows:\n",
    "\n",
    "- Mean Absolute Error (MAE) indicates average prediction error in dollars\n",
    "- Root Mean Squared Error (RMSE) penalizes larger prediction errors\n",
    "- R² measures the proportion of variance explained by the model\n",
    "\n",
    "These results are compared directly against the baseline model.\n",
    "A successful model must demonstrate:\n",
    "- Lower MAE than baseline\n",
    "- Lower RMSE than baseline\n",
    "- Positive R² value\n",
    "\n",
    "The Linear Regression results will guide whether regularization or more complex models are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe30a0",
   "metadata": {},
   "source": [
    "## Leakage Diagnosis\n",
    "\n",
    "The near-perfect Linear Regression performance indicates the presence of target leakage.\n",
    "\n",
    "Some input features directly determine SalesAmount, which allows the model to effectively reproduce the target rather than learn general predictive patterns.\n",
    "\n",
    "To ensure a fair and realistic evaluation, features that are deterministically linked to the target or only known post-transaction must be removed before modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4875477e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48318, 460), (12080, 460))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicitly remove target leakage features\n",
    "leakage_features = [\n",
    "    \"UnitPrice\",\n",
    "    \"UnitPriceDiscountPct\",\n",
    "    \"DiscountAmount\",\n",
    "    \"OrderQuantity\"\n",
    "]\n",
    "\n",
    "X_train_clean = X_train.drop(columns=leakage_features)\n",
    "X_test_clean = X_test.drop(columns=leakage_features)\n",
    "\n",
    "X_train_clean.shape, X_test_clean.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc63e000",
   "metadata": {},
   "source": [
    "## Linear Regression (Leakage-Free Features)\n",
    "\n",
    "After removing leakage-prone features, the Linear Regression model is retrained using only information that would be available at prediction time.\n",
    "\n",
    "This provides a realistic estimate of model performance and ensures comparability with future models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac19a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train Linear Regression without leakage\n",
    "lin_reg_clean = LinearRegression()\n",
    "lin_reg_clean.fit(X_train_clean, y_train)\n",
    "\n",
    "y_pred_lr_clean = lin_reg_clean.predict(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38a7160c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016972559631497124, np.float64(0.06979550105867736), 0.9999999943798626)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clean_mae = mean_absolute_error(y_test, y_pred_lr_clean)\n",
    "lr_clean_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr_clean))\n",
    "lr_clean_r2 = r2_score(y_test, y_pred_lr_clean)\n",
    "\n",
    "lr_clean_mae, lr_clean_rmse, lr_clean_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af0ac8",
   "metadata": {},
   "source": [
    "## Extended Leakage Mitigation\n",
    "\n",
    "Further inspection revealed additional features that indirectly encode pricing or transaction-level information closely tied to `SalesAmount`.\n",
    "\n",
    "To ensure a realistic prediction setting, all price-related and post-transaction attributes are removed. The remaining feature set reflects information that would be available prior to or at order time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf2ba856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48318, 453), (12080, 453))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove additional leakage-prone features\n",
    "additional_leakage_features = [\n",
    "    \"DealerPrice\",\n",
    "    \"ListPrice\",\n",
    "    \"StandardCost\",\n",
    "    \"ProductStandardCost\",\n",
    "    \"Freight\",\n",
    "    \"RevisionNumber\",\n",
    "    \"ProductSubcategoryKey\"\n",
    "]\n",
    "\n",
    "X_train_final = X_train_clean.drop(columns=additional_leakage_features)\n",
    "X_test_final = X_test_clean.drop(columns=additional_leakage_features)\n",
    "\n",
    "X_train_final.shape, X_test_final.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934f48c",
   "metadata": {},
   "source": [
    "## Linear Regression (Fully Leakage-Free)\n",
    "\n",
    "After removing all identified leakage sources, Linear Regression is trained using only non-transactional, non-price-related features.\n",
    "\n",
    "This model provides a realistic baseline for predictive performance and serves as a reference for regularized and nonlinear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d34152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.8318416250645018, np.float64(9.234553568611803), 0.9999016161342982)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg_final = LinearRegression()\n",
    "lin_reg_final.fit(X_train_final, y_train)\n",
    "\n",
    "y_pred_lr_final = lin_reg_final.predict(X_test_final)\n",
    "\n",
    "lr_final_mae = mean_absolute_error(y_test, y_pred_lr_final)\n",
    "lr_final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lr_final))\n",
    "lr_final_r2 = r2_score(y_test, y_pred_lr_final)\n",
    "\n",
    "lr_final_mae, lr_final_rmse, lr_final_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f4720",
   "metadata": {},
   "source": [
    "## Interpretation of Final Model Performance\n",
    "\n",
    "Despite extensive leakage mitigation, the Linear Regression model achieves a very high R² score.\n",
    "\n",
    "This performance is explained by the structure of the dataset rather than residual target leakage:\n",
    "\n",
    "- The prediction task operates at the transaction / line-item level\n",
    "- High-cardinality categorical features (e.g. product identifiers, customer attributes) allow the model to learn stable pricing patterns\n",
    "- Sales amounts are largely deterministic within this dataset once product and customer context is known\n",
    "\n",
    "As a result, the model effectively learns strong associations rather than forecasting uncertain outcomes.\n",
    "\n",
    "This behavior is acceptable for explaining relationships within the data, but it should be interpreted with caution if used for future predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f6bb8",
   "metadata": {},
   "source": [
    "## Ridge Regression — Conceptual Overview\n",
    "\n",
    "While Linear Regression fits coefficients by minimizing prediction error, it can become unstable when the number of features is large or when features are highly correlated.\n",
    "\n",
    "Ridge Regression extends Linear Regression by adding **L2 regularization**, which penalizes large coefficient values during training.\n",
    "\n",
    "Conceptually, Ridge Regression:\n",
    "\n",
    "- Preserves the linear relationship between features and target\n",
    "- Shrinks coefficients toward zero (but does not eliminate them)\n",
    "- Reduces model sensitivity to correlated and high-dimensional features\n",
    "- Improves numerical stability and generalization\n",
    "\n",
    "This makes Ridge Regression well-suited for datasets created through extensive one-hot encoding, as is the case in this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982dc27c",
   "metadata": {},
   "source": [
    "## Ridge Regression — Model Training\n",
    "\n",
    "We now train a Ridge Regression model using the fully leakage-free feature set.\n",
    "\n",
    "A fixed regularization strength (`alpha`) is used initially to assess whether coefficient shrinkage improves robustness relative to standard Linear Regression.\n",
    "\n",
    "The `alpha` parameter controls regularization strength:\n",
    "\n",
    "- Small alpha → behaves similarly to Linear Regression\n",
    "- Large alpha → stronger penalty, more coefficient shrinkage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "854efdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ridge Regression model\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "# Train Ridge Regression model\n",
    "ridge_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_ridge = ridge_model.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07794834",
   "metadata": {},
   "source": [
    "## Ridge Regression — Evaluation\n",
    "\n",
    "Ridge Regression is evaluated using the same metrics as previous models to ensure direct comparability.\n",
    "\n",
    "This evaluation helps determine whether regularization:\n",
    "\n",
    "- Maintains predictive accuracy\n",
    "- Improves robustness\n",
    "- Provides a safer alternative to unregularized Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a7e8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.96459229842796, np.float64(9.271238811862148), 0.999900832900966)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "ridge_rmse = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "ridge_r2 = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "ridge_mae, ridge_rmse, ridge_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc739da",
   "metadata": {},
   "source": [
    "## Linear vs Ridge Regression — Model Comparison\n",
    "\n",
    "Linear Regression and Ridge Regression are compared using identical training data and evaluation metrics.\n",
    "\n",
    "Key considerations:\n",
    "\n",
    "- Whether Ridge maintains comparable predictive performance\n",
    "- Whether regularization provides additional robustness\n",
    "- Whether coefficient shrinkage is desirable given the dataset structure\n",
    "\n",
    "Even if Ridge does not outperform Linear Regression numerically, its regularization offers improved stability and safer generalization, making it preferable in high-dimensional settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7625ce1a",
   "metadata": {},
   "source": [
    "## Model Performance Comparison\n",
    "\n",
    "To summarize model performance, the baseline, Linear Regression, and Ridge Regression models are compared using consistent evaluation metrics.\n",
    "\n",
    "This comparison highlights the magnitude of improvement over the baseline and assesses whether regularization affects predictive performance in a high-dimensional setting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b8c7546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline (Mean Predictor)</td>\n",
       "      <td>694.756261</td>\n",
       "      <td>931.022667</td>\n",
       "      <td>-0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression (Final)</td>\n",
       "      <td>2.831842</td>\n",
       "      <td>9.234554</td>\n",
       "      <td>0.999902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>2.964592</td>\n",
       "      <td>9.271239</td>\n",
       "      <td>0.999901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model         MAE        RMSE        R2\n",
       "0  Baseline (Mean Predictor)  694.756261  931.022667 -0.000029\n",
       "1  Linear Regression (Final)    2.831842    9.234554  0.999902\n",
       "2           Ridge Regression    2.964592    9.271239  0.999901"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create comparison table\n",
    "model_comparison = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Baseline (Mean Predictor)\",\n",
    "        \"Linear Regression (Final)\",\n",
    "        \"Ridge Regression\"\n",
    "    ],\n",
    "    \"MAE\": [\n",
    "        baseline_mae,\n",
    "        lr_final_mae,\n",
    "        ridge_mae\n",
    "    ],\n",
    "    \"RMSE\": [\n",
    "        baseline_rmse,\n",
    "        lr_final_rmse,\n",
    "        ridge_rmse\n",
    "    ],\n",
    "    \"R2\": [\n",
    "        baseline_r2,\n",
    "        lr_final_r2,\n",
    "        ridge_r2\n",
    "    ]\n",
    "})\n",
    "\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213bb6b6",
   "metadata": {},
   "source": [
    "## Final Model Selection\n",
    "\n",
    "Both Linear Regression and Ridge Regression significantly outperform the baseline model across all evaluation metrics.\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- Linear Regression achieves the lowest error and highest R²\n",
    "- Ridge Regression produces nearly identical performance\n",
    "- Regularization does not materially improve accuracy for this dataset\n",
    "\n",
    "Given these results, **Linear Regression** is selected as the final model due to its simplicity, interpretability, and marginally better accuracy.\n",
    "\n",
    "Ridge Regression remains a strong alternative in scenarios where model stability or coefficient shrinkage is prioritized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbae0b",
   "metadata": {},
   "source": [
    "## Limitations and Considerations\n",
    "\n",
    "While model performance is strong, several limitations should be noted:\n",
    "\n",
    "- The prediction task operates at a transactional level, where outcomes are largely deterministic once context is known\n",
    "- High-cardinality categorical features can lead the model to rely heavily on patterns specific to this dataset.\n",
    "- Performance may not generalize to future pricing, promotions, or unseen products\n",
    "\n",
    "As a result, the model is best suited for **explanatory analysis** rather than long-horizon forecasting.\n",
    "\n",
    "Future work could include:\n",
    "\n",
    "- Temporal validation strategies\n",
    "- Product-level aggregation\n",
    "- Regularized or tree-based models for robustness testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
