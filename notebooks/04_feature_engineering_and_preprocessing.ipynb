{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "760fd9f6",
   "metadata": {},
   "source": [
    "# Feature Engineering and Preprocessing\n",
    "\n",
    "This notebook prepares the analytics-ready dataset for machine learning modeling using a **hybrid PySpark–pandas workflow**.\n",
    "\n",
    "PySpark is used for scalable data loading, joining fact and dimension tables, and handling operations that benefit from distributed processing. After these steps, the dataset is converted to pandas for feature engineering and preprocessing steps that integrate naturally with common machine learning libraries.\n",
    "\n",
    "This design allows us to demonstrate the use of big data tools while maintaining compatibility with standard modeling workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453e86b",
   "metadata": {},
   "source": [
    "## Feature Engineering Workflow\n",
    "\n",
    "The preprocessing pipeline in this notebook follows these stages:\n",
    "\n",
    "1. Load fact and dimension tables using PySpark\n",
    "2. Join fact and dimension tables into a single Spark DataFrame\n",
    "3. Remove columns with extreme missingness using Spark operations\n",
    "4. Validate schema and record counts\n",
    "5. Convert the Spark DataFrame to pandas for downstream processing\n",
    "6. Define the target variable for modeling\n",
    "7. Handle missing values and data inconsistencies\n",
    "8. Transform skewed numerical features\n",
    "9. Encode categorical variables\n",
    "10. Split the data into training and testing sets\n",
    "\n",
    "Each step is documented to ensure transparency, reproducibility, and alignment with both big data processing and machine learning requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b1a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e503ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/09 05:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"FeatureEngineering\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc9012f",
   "metadata": {},
   "source": [
    "To avoid duplicating path-handling logic across notebooks, a helper module (`data_utils.py`) is used.\n",
    "\n",
    "In this notebook, the `data_dir()` function is used to reliably resolve the relative path to the raw data directory. This approach improves reproducibility and keeps environment-specific logic out of the analysis code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42cde4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/raw'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure workspace root is on sys.path so `src` package is importable\n",
    "workspace_root = os.path.abspath('..')\n",
    "\n",
    "if workspace_root not in sys.path:\n",
    "    sys.path.insert(0, workspace_root)\n",
    "    \n",
    "import src.data_utils as du\n",
    "\n",
    "# Resolve DATA_DIR from the package helper and expose as variable\n",
    "DATA_DIR = du.data_dir()\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a74582",
   "metadata": {},
   "source": [
    "Below, the raw fact and dimension tables are loaded from CSV files using **PySpark**.\n",
    "\n",
    "Using Spark at this stage allows us to demonstrate scalable data ingestion and joining logic that would generalize to larger-than-memory datasets. These tables are then joined to recreate the integrated dataset used for feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a3443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the joined, analytics-ready dataset\n",
    "fact_sales = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    os.path.join(DATA_DIR, \"FactInternetSales.csv\")\n",
    ")\n",
    "\n",
    "# Load key dimension tables\n",
    "dim_product = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    os.path.join(DATA_DIR, \"DimProduct.csv\")\n",
    ")\n",
    "\n",
    "dim_customer = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    os.path.join(DATA_DIR, \"DimCustomer.csv\")\n",
    ")\n",
    "\n",
    "dim_date = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    os.path.join(DATA_DIR, \"DimDate.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "155c43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join fact table with dimension tables\n",
    "sales_df = (\n",
    "    fact_sales\n",
    "    .join(dim_product, on=\"ProductKey\", how=\"left\")\n",
    "    .join(dim_customer, on=\"CustomerKey\", how=\"left\")\n",
    "    .join(dim_date, fact_sales.OrderDateKey == dim_date.DateKey, how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e55018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with extreme missingness (Spark-side)\n",
    "row_count = sales_df.count()\n",
    "\n",
    "missing_ratios = {\n",
    "    c: sales_df.filter(col(c).isNull()).count() / row_count\n",
    "    for c in sales_df.columns\n",
    "}\n",
    "\n",
    "cols_to_drop = [c for c, r in missing_ratios.items() if r >= 0.99]\n",
    "\n",
    "sales_df = sales_df.drop(*cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e364e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerKey: integer (nullable = true)\n",
      " |-- ProductKey: integer (nullable = true)\n",
      " |-- OrderDateKey: integer (nullable = true)\n",
      " |-- DueDateKey: integer (nullable = true)\n",
      " |-- ShipDateKey: integer (nullable = true)\n",
      " |-- PromotionKey: integer (nullable = true)\n",
      " |-- CurrencyKey: integer (nullable = true)\n",
      " |-- SalesTerritoryKey: integer (nullable = true)\n",
      " |-- SalesOrderNumber: string (nullable = true)\n",
      " |-- SalesOrderLineNumber: integer (nullable = true)\n",
      " |-- RevisionNumber: integer (nullable = true)\n",
      " |-- OrderQuantity: integer (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- ExtendedAmount: double (nullable = true)\n",
      " |-- UnitPriceDiscountPct: integer (nullable = true)\n",
      " |-- DiscountAmount: integer (nullable = true)\n",
      " |-- ProductStandardCost: double (nullable = true)\n",
      " |-- TotalProductCost: double (nullable = true)\n",
      " |-- SalesAmount: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- Freight: double (nullable = true)\n",
      " |-- CarrierTrackingNumber: string (nullable = true)\n",
      " |-- CustomerPONumber: string (nullable = true)\n",
      " |-- OrderDate: timestamp (nullable = true)\n",
      " |-- DueDate: timestamp (nullable = true)\n",
      " |-- ShipDate: timestamp (nullable = true)\n",
      " |-- ProductAlternateKey: string (nullable = true)\n",
      " |-- ProductSubcategoryKey: integer (nullable = true)\n",
      " |-- WeightUnitMeasureCode: string (nullable = true)\n",
      " |-- SizeUnitMeasureCode: string (nullable = true)\n",
      " |-- EnglishProductName: string (nullable = true)\n",
      " |-- SpanishProductName: string (nullable = true)\n",
      " |-- FrenchProductName: string (nullable = true)\n",
      " |-- StandardCost: double (nullable = true)\n",
      " |-- FinishedGoodsFlag: boolean (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- SafetyStockLevel: integer (nullable = true)\n",
      " |-- ReorderPoint: integer (nullable = true)\n",
      " |-- ListPrice: double (nullable = true)\n",
      " |-- Size: string (nullable = true)\n",
      " |-- SizeRange: string (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- DaysToManufacture: integer (nullable = true)\n",
      " |-- ProductLine: string (nullable = true)\n",
      " |-- DealerPrice: double (nullable = true)\n",
      " |-- Class: string (nullable = true)\n",
      " |-- Style: string (nullable = true)\n",
      " |-- ModelName: string (nullable = true)\n",
      " |-- EnglishDescription: string (nullable = true)\n",
      " |-- StartDate: string (nullable = true)\n",
      " |-- EndDate: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- GeographyKey: integer (nullable = true)\n",
      " |-- CustomerAlternateKey: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- MiddleName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- NameStyle: boolean (nullable = true)\n",
      " |-- BirthDate: date (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- EmailAddress: string (nullable = true)\n",
      " |-- YearlyIncome: integer (nullable = true)\n",
      " |-- TotalChildren: integer (nullable = true)\n",
      " |-- NumberChildrenAtHome: integer (nullable = true)\n",
      " |-- EnglishEducation: string (nullable = true)\n",
      " |-- SpanishEducation: string (nullable = true)\n",
      " |-- FrenchEducation: string (nullable = true)\n",
      " |-- EnglishOccupation: string (nullable = true)\n",
      " |-- SpanishOccupation: string (nullable = true)\n",
      " |-- FrenchOccupation: string (nullable = true)\n",
      " |-- HouseOwnerFlag: integer (nullable = true)\n",
      " |-- NumberCarsOwned: integer (nullable = true)\n",
      " |-- AddressLine1: string (nullable = true)\n",
      " |-- AddressLine2: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- DateFirstPurchase: date (nullable = true)\n",
      " |-- CommuteDistance: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60398"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect schema and row count (Spark DataFrame)\n",
    "sales_df.printSchema()\n",
    "sales_df.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb028cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to pandas for downstream processing\n",
    "sales_df = sales_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774d517",
   "metadata": {},
   "source": [
    "## Dataset Reload Validation\n",
    "\n",
    "After joining the fact and dimension tables using PySpark, the resulting Spark DataFrame is inspected to confirm schema correctness and row counts.\n",
    "\n",
    "Once this validation step is complete, the dataset is converted to a pandas DataFrame. All subsequent feature engineering and preprocessing steps are performed in pandas to ensure compatibility with standard machine learning libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a210c9",
   "metadata": {},
   "source": [
    "## Feature Categorization — How We Decide\n",
    "\n",
    "\n",
    "Before performing any transformations, features are grouped into categories based on their role in the business process and their availability at prediction time.\n",
    "\n",
    "The guiding principle used for this categorization is:\n",
    "\n",
    "> *A feature may only be used for prediction if it would be known at the time the prediction is made.*\n",
    "\n",
    "Each category below documents **why** certain columns belong to it.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Identifier and Technical Columns\n",
    "\n",
    "**Definition:**  \n",
    "Columns that uniquely identify records or entities but do not carry predictive meaning.\n",
    "\n",
    "**Examples:**  \n",
    "- ProductKey  \n",
    "- CustomerKey  \n",
    "- SalesOrderNumber  \n",
    "\n",
    "**Reasoning:**  \n",
    "These columns act as database identifiers rather than informative attributes. Their numeric values are arbitrary and are unlikely to contribute meaningful predictive signal. Including them would cause the model to learn meaningless patterns.\n",
    "\n",
    "**Action:**  \n",
    "Excluded from modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Leakage-Prone (Post-Outcome) Columns\n",
    "\n",
    "**Definition:**  \n",
    "Columns that are calculated after the transaction completes or are derived directly from the target variable.\n",
    "\n",
    "**Examples:**  \n",
    "- ExtendedAmount  \n",
    "- TotalProductCost  \n",
    "- TaxAmt  \n",
    "\n",
    "**Reasoning:**  \n",
    "These values are only known *after* the sale has occurred and the final SalesAmount is determined. Including them would leak future information into the model, resulting in unrealistically high performance that would not generalize to real-world predictions.\n",
    "\n",
    "**Action:**  \n",
    "Excluded to prevent data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Numerical Predictive Features\n",
    "\n",
    "**Definition:**  \n",
    "Quantitative attributes that are known **before** or **at the time of the sale**.\n",
    "\n",
    "**Examples:**  \n",
    "- OrderQuantity  \n",
    "- UnitPrice  \n",
    "- UnitPriceDiscountPct  \n",
    "\n",
    "**Reasoning:**  \n",
    "These features directly influence the transaction outcome and are available at prediction time. They represent legitimate inputs that the model can reasonably use to estimate SalesAmount.\n",
    "\n",
    "**Action:**  \n",
    "Retained for modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Categorical Descriptive Features\n",
    "\n",
    "**Definition:**  \n",
    "Non-numeric attributes describing products, customers, or time periods.\n",
    "\n",
    "**Examples:**  \n",
    "- ProductCategory  \n",
    "- EnglishMonthName  \n",
    "- CustomerGender  \n",
    "\n",
    "**Reasoning:**  \n",
    "Although not numerical, these features encode meaningful contextual information. They will be transformed into numerical representations using encoding techniques suitable for machine learning models.\n",
    "\n",
    "**Action:**  \n",
    "Retained and encoded in later steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8e7fe",
   "metadata": {},
   "source": [
    "## Target Variable Definition\n",
    "\n",
    "The objective of this modeling task is to predict **SalesAmount** at the transaction (line-item) level based on product, customer, and time-related attributes.\n",
    "\n",
    "`SalesAmount` is selected as the target variable because:\n",
    "- It represents the primary business outcome (revenue)\n",
    "- It is continuous and suitable for regression-based models\n",
    "- It showed meaningful patterns and skewness during EDA\n",
    "- It aligns naturally with retail and sales forecasting use cases\n",
    "\n",
    "All remaining features will be evaluated as potential predictors of `SalesAmount`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33ac7237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60398, 77), (60398,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define target variable\n",
    "TARGET_COL = \"SalesAmount\"\n",
    "\n",
    "# Separate target variable\n",
    "y = sales_df[TARGET_COL]\n",
    "\n",
    "# Create initial feature set by dropping target\n",
    "X = sales_df.drop(columns=[TARGET_COL])\n",
    "\n",
    "# Basic validation\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1f230",
   "metadata": {},
   "source": [
    "## Feature Categorization — Applying the Decision Rules\n",
    "\n",
    "The categories below illustrate how features are grouped based on the decision rules described above. At this stage, we identify representative examples rather than exhaustively classifying all columns. Final feature selection will be performed programmatically in subsequent steps.\n",
    "\n",
    "To prepare the dataset for machine learning, features are grouped into the following categories:\n",
    "\n",
    "### 1. Identifier and Technical Columns\n",
    "These columns uniquely identify records or serve operational purposes and do not carry predictive value. Examples include:\n",
    "- SalesOrderNumber\n",
    "- SalesOrderLineNumber\n",
    "- Surrogate keys (e.g., ProductKey, CustomerKey)\n",
    "\n",
    "These will be excluded from modeling.\n",
    "\n",
    "### 2. Leakage-Prone Columns\n",
    "These columns are derived directly from or mathematically linked to the target variable (`SalesAmount`) and would cause data leakage if included. Examples include:\n",
    "- ExtendedAmount\n",
    "- TotalProductCost\n",
    "- TaxAmt\n",
    "\n",
    "These are excluded from the feature set prior to modeling.\n",
    "\n",
    "### 3. Numerical Predictive Features\n",
    "Continuous or discrete numerical attributes that may explain variations in sales, such as:\n",
    "- OrderQuantity\n",
    "- UnitPrice\n",
    "- Discount-related fields\n",
    "- Product cost attributes\n",
    "\n",
    "These features may require transformation due to skewness or scale differences.\n",
    "\n",
    "### 4. Categorical Features\n",
    "Non-numerical attributes describing products, customers, and time, such as:\n",
    "- Product categories\n",
    "- Customer demographics\n",
    "- Calendar attributes (Year, Month, Quarter)\n",
    "\n",
    "These will be encoded into numerical form in later steps.\n",
    "\n",
    "This structured categorization ensures transparent, defensible feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770af94e",
   "metadata": {},
   "source": [
    "## Missing Value Analysis\n",
    "\n",
    "Before applying any preprocessing steps, we assess the presence and extent of missing values across all features. This analysis helps determine whether missing data is negligible, requires imputation, or warrants column removal.\n",
    "\n",
    "Understanding missingness patterns ensures that subsequent preprocessing decisions are data-driven and defensible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a96603a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AddressLine2</th>\n",
       "      <td>59293</td>\n",
       "      <td>98.170469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EndDate</th>\n",
       "      <td>54970</td>\n",
       "      <td>91.012947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeightUnitMeasureCode</th>\n",
       "      <td>45193</td>\n",
       "      <td>74.825325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SizeUnitMeasureCode</th>\n",
       "      <td>45193</td>\n",
       "      <td>74.825325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight</th>\n",
       "      <td>45193</td>\n",
       "      <td>74.825325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>38946</td>\n",
       "      <td>64.482268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Size</th>\n",
       "      <td>37549</td>\n",
       "      <td>62.169277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Style</th>\n",
       "      <td>36092</td>\n",
       "      <td>59.756946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MiddleName</th>\n",
       "      <td>25495</td>\n",
       "      <td>42.211663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SpanishProductName</th>\n",
       "      <td>13135</td>\n",
       "      <td>21.747409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FrenchProductName</th>\n",
       "      <td>13135</td>\n",
       "      <td>21.747409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>5428</td>\n",
       "      <td>8.987053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       missing_count  missing_percent\n",
       "AddressLine2                   59293        98.170469\n",
       "EndDate                        54970        91.012947\n",
       "WeightUnitMeasureCode          45193        74.825325\n",
       "SizeUnitMeasureCode            45193        74.825325\n",
       "Weight                         45193        74.825325\n",
       "Class                          38946        64.482268\n",
       "Size                           37549        62.169277\n",
       "Style                          36092        59.756946\n",
       "MiddleName                     25495        42.211663\n",
       "SpanishProductName             13135        21.747409\n",
       "FrenchProductName              13135        21.747409\n",
       "Status                          5428         8.987053"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate missing values per column\n",
    "missing_counts = sales_df.isna().sum()\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "missing_percent = (missing_counts / len(sales_df)) * 100\n",
    "\n",
    "# Combine into a summary DataFrame\n",
    "missing_summary = (\n",
    "    pd.DataFrame({\n",
    "        \"missing_count\": missing_counts,\n",
    "        \"missing_percent\": missing_percent\n",
    "    })\n",
    "    .query(\"missing_count > 0\")\n",
    "    .sort_values(by=\"missing_percent\", ascending=False)\n",
    ")\n",
    "\n",
    "missing_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f1403",
   "metadata": {},
   "source": [
    "## Missing Value Interpretation and Handling Strategy\n",
    "\n",
    "The missing value analysis reveals distinct patterns driven by the data warehouse schema and business logic rather than random data quality issues.\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- Several columns exhibit near-total missingness (≈ 100%), particularly operational fields and descriptive date attributes. These columns provide no usable signal and will be removed.\n",
    "\n",
    "- Product-related attributes such as size, weight, style, and class show high but systematic missingness, reflecting optional or category-specific characteristics. These features will be evaluated individually and either dropped or encoded with explicit missing indicators.\n",
    "\n",
    "- A small number of attributes exhibit moderate missingness and may be retained through imputation or categorical encoding.\n",
    "\n",
    "Based on these findings, features with extreme missingness will be removed prior to modeling, while others will be handled using controlled preprocessing techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40138152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60398, 77)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns with extremely high missingness (>= 99%)\n",
    "missing_threshold = 0.99\n",
    "\n",
    "cols_to_drop = (\n",
    "    missing_summary[missing_summary[\"missing_percent\"] >= missing_threshold * 100]\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Drop from feature set only (not target)\n",
    "X = X.drop(columns=cols_to_drop)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9aeba",
   "metadata": {},
   "source": [
    "## Handling Remaining Missing Values\n",
    "\n",
    "After removing columns with extreme missingness, the remaining missing values represent meaningful absence rather than data corruption.\n",
    "\n",
    "These missing values are handled differently depending on feature type:\n",
    "\n",
    "- **Numerical features**: Missing values are imputed using the median to preserve distribution robustness in the presence of skewness.\n",
    "- **Categorical features**: Missing values are treated as an explicit category (\"Unknown\") to preserve information about absence.\n",
    "\n",
    "This approach ensures no rows are dropped while maintaining model compatibility and interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05978c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddressLine2             59293\n",
       "EndDate                  54970\n",
       "WeightUnitMeasureCode    45193\n",
       "SizeUnitMeasureCode      45193\n",
       "Weight                   45193\n",
       "Class                    38946\n",
       "Size                     37549\n",
       "Style                    36092\n",
       "MiddleName               25495\n",
       "SpanishProductName       13135\n",
       "FrenchProductName        13135\n",
       "Status                    5428\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify remaining missing values after column removal\n",
    "remaining_missing = X.isna().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0]\n",
    "\n",
    "remaining_missing.sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a65b99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Weight'],\n",
       " ['WeightUnitMeasureCode',\n",
       "  'SizeUnitMeasureCode',\n",
       "  'SpanishProductName',\n",
       "  'FrenchProductName',\n",
       "  'Size',\n",
       "  'Class',\n",
       "  'Style',\n",
       "  'EndDate',\n",
       "  'Status',\n",
       "  'MiddleName',\n",
       "  'AddressLine2'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate numerical and categorical columns with missing values\n",
    "num_missing_cols = X.select_dtypes(include=[np.number]).columns\n",
    "num_missing_cols = [col for col in num_missing_cols if X[col].isna().any()]\n",
    "\n",
    "cat_missing_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "cat_missing_cols = [col for col in cat_missing_cols if X[col].isna().any()]\n",
    "\n",
    "num_missing_cols, cat_missing_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78be589",
   "metadata": {},
   "source": [
    "### Missing Value Treatment Strategy\n",
    "\n",
    "Remaining missing values fall into two categories:\n",
    "\n",
    "**Numerical Features**\n",
    "- Missingness is handled using median imputation.\n",
    "- Median is preferred over mean due to skewed distributions observed in EDA.\n",
    "\n",
    "**Categorical Features**\n",
    "- Missing values are replaced with the category \"Unknown\".\n",
    "- This preserves information that the value was absent rather than discarding records.\n",
    "\n",
    "This strategy avoids data loss while maintaining statistical stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3de446d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute numerical features with median\n",
    "for col in num_missing_cols:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Impute categorical features with explicit label\n",
    "for col in cat_missing_cols:\n",
    "    X[col] = X[col].fillna(\"Unknown\")\n",
    "\n",
    "# Validate no remaining missing values\n",
    "X.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eedc476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60398, 65)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop identifier, surrogate key, and leakage-prone columns\n",
    "cols_to_drop = [\n",
    "    # Leakage-prone\n",
    "    \"ExtendedAmount\",\n",
    "    \"TotalProductCost\",\n",
    "    \"TaxAmt\",\n",
    "\n",
    "    # Identifiers / surrogate keys\n",
    "    \"ProductKey\",\n",
    "    \"CustomerKey\",\n",
    "    \"PromotionKey\",\n",
    "    \"CurrencyKey\",\n",
    "    \"SalesTerritoryKey\",\n",
    "    \"SalesOrderLineNumber\",\n",
    "\n",
    "    # Date keys (handled via DimDate attributes instead)\n",
    "    \"OrderDateKey\",\n",
    "    \"DueDateKey\",\n",
    "    \"ShipDateKey\"\n",
    "]\n",
    "\n",
    "X = X.drop(columns=[col for col in cols_to_drop if col in X.columns])\n",
    "\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e7a90",
   "metadata": {},
   "source": [
    "## Numerical Feature Transformation\n",
    "\n",
    "After converting the dataset to pandas, numerical features are examined for skewness, as observed during the exploratory data analysis (EDA) phase.\n",
    "\n",
    "To improve model performance and numerical stability, selected skewed numerical features are transformed using a logarithmic transformation.\n",
    "\n",
    "Log transformation:\n",
    "\n",
    "- Reduces skewness\n",
    "- Compresses extreme values\n",
    "- Improves compatibility with linear and distance-based models\n",
    "\n",
    "Only strictly positive numerical features are transformed to avoid mathematical issues. Identifier fields and leakage-prone variables are excluded from transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "434dfeae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21,\n",
       " Index(['RevisionNumber', 'OrderQuantity', 'UnitPrice', 'UnitPriceDiscountPct',\n",
       "        'DiscountAmount', 'ProductStandardCost', 'Freight',\n",
       "        'ProductSubcategoryKey', 'StandardCost', 'SafetyStockLevel',\n",
       "        'ReorderPoint', 'ListPrice', 'Weight', 'DaysToManufacture',\n",
       "        'DealerPrice', 'GeographyKey', 'YearlyIncome', 'TotalChildren',\n",
       "        'NumberChildrenAtHome', 'HouseOwnerFlag', 'NumberCarsOwned'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify numerical features\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "len(numerical_cols), numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88fba9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductStandardCost      1.950547\n",
       "StandardCost             1.950547\n",
       "Freight                  1.927515\n",
       "ListPrice                1.927515\n",
       "UnitPrice                1.927515\n",
       "DealerPrice              1.927515\n",
       "NumberChildrenAtHome     1.281429\n",
       "DaysToManufacture        1.144009\n",
       "ReorderPoint             1.126946\n",
       "SafetyStockLevel         1.126946\n",
       "YearlyIncome             0.783957\n",
       "GeographyKey             0.734562\n",
       "Weight                   0.550952\n",
       "TotalChildren            0.463057\n",
       "NumberCarsOwned          0.402790\n",
       "OrderQuantity            0.000000\n",
       "RevisionNumber           0.000000\n",
       "DiscountAmount           0.000000\n",
       "UnitPriceDiscountPct     0.000000\n",
       "ProductSubcategoryKey   -0.688218\n",
       "HouseOwnerFlag          -0.823695\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate skewness of numerical features\n",
    "skewness = X[numerical_cols].skew().sort_values(ascending=False)\n",
    "\n",
    "skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f482d9",
   "metadata": {},
   "source": [
    "### Skewness-Based Transformation Criteria\n",
    "\n",
    "Numerical features with an absolute skewness greater than 1 are considered highly skewed and are selected for transformation.\n",
    "\n",
    "This threshold balances correction of extreme distributions while preserving natural variability in approximately symmetric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a4c47cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ['ProductStandardCost',\n",
       "  'StandardCost',\n",
       "  'Freight',\n",
       "  'ListPrice',\n",
       "  'UnitPrice',\n",
       "  'DealerPrice',\n",
       "  'NumberChildrenAtHome',\n",
       "  'DaysToManufacture',\n",
       "  'ReorderPoint',\n",
       "  'SafetyStockLevel'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select highly skewed numerical features\n",
    "skewed_cols = skewness[abs(skewness) > 1].index.tolist()\n",
    "\n",
    "len(skewed_cols), skewed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9bdf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log1p transformation to skewed numerical features\n",
    "for col in skewed_cols:\n",
    "    X[col] = np.log1p(X[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1325d3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Freight                 1.169011\n",
       "DaysToManufacture       1.144009\n",
       "StandardCost            0.807867\n",
       "ProductStandardCost     0.807867\n",
       "NumberChildrenAtHome    0.807117\n",
       "DealerPrice             0.774139\n",
       "UnitPrice               0.740682\n",
       "ListPrice               0.740682\n",
       "ReorderPoint            0.296094\n",
       "SafetyStockLevel        0.289817\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recalculate skewness after transformation\n",
    "post_skewness = X[skewed_cols].skew().sort_values(ascending=False)\n",
    "\n",
    "post_skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f3c23",
   "metadata": {},
   "source": [
    "## Categorical Feature Encoding\n",
    "\n",
    "After completing Spark-based data preparation and pandas-based feature engineering, categorical features are encoded into numerical form so they can be used by machine learning models.\n",
    "\n",
    "Encoding strategy:\n",
    "\n",
    "- Categorical features are one-hot encoded using pandas\n",
    "- High-cardinality identifiers and free-text fields have already been removed\n",
    "- Missing values have already been handled using explicit categories\n",
    "\n",
    "This approach preserves categorical information while avoiding unintended ordinal relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdcb00e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44,\n",
       " Index(['SalesOrderNumber', 'CarrierTrackingNumber', 'CustomerPONumber',\n",
       "        'OrderDate', 'DueDate', 'ShipDate', 'ProductAlternateKey',\n",
       "        'WeightUnitMeasureCode', 'SizeUnitMeasureCode', 'EnglishProductName',\n",
       "        'SpanishProductName', 'FrenchProductName', 'FinishedGoodsFlag', 'Color',\n",
       "        'Size', 'SizeRange', 'ProductLine', 'Class', 'Style', 'ModelName',\n",
       "        'EnglishDescription', 'StartDate', 'EndDate', 'Status',\n",
       "        'CustomerAlternateKey', 'FirstName', 'MiddleName', 'LastName',\n",
       "        'NameStyle', 'BirthDate', 'MaritalStatus', 'Gender', 'EmailAddress',\n",
       "        'EnglishEducation', 'SpanishEducation', 'FrenchEducation',\n",
       "        'EnglishOccupation', 'SpanishOccupation', 'FrenchOccupation',\n",
       "        'AddressLine1', 'AddressLine2', 'Phone', 'DateFirstPurchase',\n",
       "        'CommuteDistance'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify categorical features\n",
    "categorical_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "len(categorical_cols), categorical_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2671fb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60398, 46)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop_cat = [\n",
    "    \"SalesOrderNumber\",\n",
    "    \"ProductAlternateKey\",\n",
    "    \"CustomerAlternateKey\",\n",
    "    \"EmailAddress\",\n",
    "    \"Phone\",\n",
    "    \"OrderDate\",\n",
    "    \"DueDate\",\n",
    "    \"ShipDate\",\n",
    "    \"StartDate\",\n",
    "    \"EndDate\",\n",
    "    \"BirthDate\",\n",
    "    \"DateFirstPurchase\",\n",
    "    \"EnglishDescription\",\n",
    "    \"ModelName\",\n",
    "    \"AddressLine1\",\n",
    "    \"AddressLine2\",\n",
    "    \"FirstName\",\n",
    "    \"MiddleName\",\n",
    "    \"LastName\"\n",
    "]\n",
    "\n",
    "X = X.drop(columns=cols_to_drop_cat)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19ddcece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,\n",
       " Index(['CarrierTrackingNumber', 'CustomerPONumber', 'WeightUnitMeasureCode',\n",
       "        'SizeUnitMeasureCode', 'EnglishProductName', 'SpanishProductName',\n",
       "        'FrenchProductName', 'FinishedGoodsFlag', 'Color', 'Size', 'SizeRange',\n",
       "        'ProductLine', 'Class', 'Style', 'Status', 'NameStyle', 'MaritalStatus',\n",
       "        'Gender', 'EnglishEducation', 'SpanishEducation', 'FrenchEducation',\n",
       "        'EnglishOccupation', 'SpanishOccupation', 'FrenchOccupation',\n",
       "        'CommuteDistance'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-identify categorical columns after cleanup\n",
    "categorical_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "len(categorical_cols), categorical_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f105472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60398, 464)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode categorical features\n",
    "X_encoded = pd.get_dummies(\n",
    "    X,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167eff5e",
   "metadata": {},
   "source": [
    "## Train–Test Split\n",
    "\n",
    "The final step in preprocessing is to split the dataset into training and testing subsets.\n",
    "\n",
    "This ensures that model performance is evaluated on unseen data and helps prevent overfitting.\n",
    "\n",
    "A fixed random state is used to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e418d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48318, 464), (12080, 464), (48318,), (12080,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b342e",
   "metadata": {},
   "source": [
    "## Persist Preprocessed Data\n",
    "\n",
    "To ensure reproducibility and separation of concerns, the final training and testing datasets are saved to disk and reloaded in the modeling notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdfd3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed data directory if it doesn't exist\n",
    "processed_dir = os.path.join(DATA_DIR, \"..\", \"processed\")\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Save train-test splits\n",
    "X_train.to_csv(os.path.join(processed_dir, \"X_train.csv\"), index=False)\n",
    "X_test.to_csv(os.path.join(processed_dir, \"X_test.csv\"), index=False)\n",
    "y_train.to_csv(os.path.join(processed_dir, \"y_train.csv\"), index=False)\n",
    "y_test.to_csv(os.path.join(processed_dir, \"y_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4c2245",
   "metadata": {},
   "source": [
    "## Preprocessing Summary and Next Steps\n",
    "\n",
    "This notebook prepared the raw sales data into a clean, model-ready feature matrix using a **hybrid big data and machine learning workflow**.\n",
    "\n",
    "### Key preprocessing outcomes:\n",
    "\n",
    "- **Big data processing**:\n",
    "  - Data loaded and joined using PySpark\n",
    "  - Columns with extreme missingness removed at the Spark level\n",
    "- **Transition to pandas**:\n",
    "  - Dataset converted to pandas for feature engineering and modeling compatibility\n",
    "- **Target variable**:\n",
    "  - `SalesAmount` (continuous, line-item revenue)\n",
    "- **Feature selection**:\n",
    "  - Removed identifier and surrogate key columns\n",
    "  - Excluded leakage-prone, post-outcome variables\n",
    "  - Removed high-cardinality and non-predictive text fields\n",
    "- **Missing value handling**:\n",
    "  - Numerical features imputed using median values\n",
    "  - Categorical features imputed using an explicit `\"Unknown\"` category\n",
    "  - No rows were dropped during preprocessing\n",
    "- **Numerical transformations**:\n",
    "  - Skewed numerical features transformed using `log1p`\n",
    "- **Categorical encoding**:\n",
    "  - One-hot encoding applied with `drop_first=True`\n",
    "  - Final encoded feature space contains **464 features**\n",
    "- **Train–test split**:\n",
    "  - 80% training / 20% testing\n",
    "  - Fixed random state for reproducibility\n",
    "\n",
    "### Final dataset shapes:\n",
    "\n",
    "- Training set: `(48,318, 464)`\n",
    "- Test set: `(12,080, 464)`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
